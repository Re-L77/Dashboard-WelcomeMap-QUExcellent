# app/controllers/ml_controller.py
from fastapi import APIRouter
from pydantic import BaseModel
from transformers import AutoTokenizer

# ðŸ”¹ Creamos el router para incluirlo en main.py
router = APIRouter(prefix="/ml", tags=["Machine Learning"])

# ðŸ”¹ Cargamos un modelo de tokenizaciÃ³n (puedes cambiar el modelo)
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-multilingual-cased")

# ðŸ”¹ Definimos el modelo de entrada (lista de textos)
class TextInput(BaseModel):
    texts: list[str]

# ðŸ”¹ Endpoint de tokenizaciÃ³n
@router.post("/tokenize/")
async def tokenize_texts(data: TextInput):
    """
    Recibe una lista de textos y devuelve sus tokens.
    Ejemplo de entrada:
    {
        "texts": ["Me gustÃ³ el onboarding", "El curso fue confuso"]
    }
    """
    encoded = tokenizer(
        data.texts,
        padding=True,
        truncation=True,
        return_tensors="pt"
    )

    # Convertimos tensores a listas para devolverlos en JSON
    return {
        "input_ids": encoded["input_ids"].tolist(),
        "attention_mask": encoded["attention_mask"].tolist(),
        "tokens": [tokenizer.convert_ids_to_tokens(ids) for ids in encoded["input_ids"]]
    }
